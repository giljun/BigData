# 빅데이터 사전학습

## Clustering

- 데이터를 유사도에 의해서 k개의 그룹으로 나눈 것을 말한다.
- Clustering을 통한 활용 사례
  - 백화점 고객을 구매 상품에 따라서 클러스터링함
  - 추천 시스템에 의해 고객의 과거 패턴을 이용해서 클러스터링함
  - Gene 데이터를 유사도에 따라서 클러스터링함
  - 텍스트 문서들을 주제에 따라서 클러스터링함
- What is Clustering?
  - Clustering이 중요함. 좋은 Measure를 사용할수록 의미있는 결과값이 나온다.
- Partitional Algorithms
  - 근사적으로 다 뒤지지 않고 일부만 찾고 그것을 통해 결과를 찾아가는 알고리즘



## K-Means Clustering

- k개의 평균점
- Prev의 평균점 검사와 Next 평균점의 검사의 분류 결과가 같으면 더 이상 작업을 진행하지 않는다. 그렇지 않다면, 계속해서 그룹 검사를 수행..
- 단점
  - 클러스터의 사이즈가 크거나 작을 때, 잘 못 찾는다.
  - 공 같은 모양의 클러스터만 잘 찾는다.
  - 클러스터에서 아주 먼 곳의 평균점이 하나 존재한다면, 알고리즘의 한계로 인해 해당 부분으로 끌려가서 실제 데이터가 없는 위치를 가리키는 단점이 존재한다.
- K-Medoids
  - outliers를 보완한 방법
  - 실제 포인터로 만드는 방법



## Hierarchical Clustering

- Top-down 방식과 Bottom-up 방식이 존재
- Bottom-up 방식을 더 많이 사용한다.
- N개의 포인터가 존재하고, 모든 포인터들의 쌍에 대해서 거리를 계산한 다음에 거리가 제일 가까운 포인터를 2개를 찾아서, 그것들을 합쳐서 하나의 클러스터로 만들어서 클러스터의 개수가 줄어든다.
- K개가 되면 작업을 중단한다.
- Agglomerative Hierarchical Clustering Algorithms
  - min : 두 포인터의 거리가 최소인 클러스터를 구한다.
  - mean : 두 포인터의 평균값의 거리가 최소인 클러스터를 구한다.
  - avg : 두 클러스터에 대해 모든 거리를 구한 다음에, 평균 거리로 두 클러스터의 거리를 계산
  - max : 두 포인터의 거리가 최대인 클러스터를 구한다.

- 위에서 하는 것을 알고리즘을 어떻게 쓰느냐에 따라 성능이 달라진다.
- Single-link(min)와 Complete-link(max)
- Average-link(avg), Mean-link(mean)
- Centroid-link
  - center에 대해서 pair-wise 가장 가까운 거리를 구하여 k개가 될 때까지 merge를 하면서 진행
- Single-link
  - 점과 점 사이의 거리가 가장 가까운 점들을 merge하면서 k개의 클러스터가 될 때까지 진행하는 방법
- Average-link
  - 점과 점 사이의 평균 거리를 찾아서 가장 가까운 점들을 merge하면서 k개의 클러스터가 될 때까지 진행하는 방법



## DBSCAN Clustering Algorithms

- Density-Based Clustering Algorithms
  -  입실론이라는 범위를 준다.
  - 범위 안에 들어오는 minumpoints를 따져서 만족하지 못하면 더이상 클러스터 늘어나지 않는다.
  - 작업이 끝나면 방문하지 않은 점을 하나 선택하여 위의 작업을 반복한다.
  - epsilon과 minPoints에 의해서 density에 따라서 클러스트의 결과가 달라진다.



## EM Clustering

- Generative Model(생성 모델)
- 데이터로부터 역으로 모델을 찾는다.
- 확률에 의해서 여러 가지 경우가 발생할 수 있는데, 그 중에 가장 확률이 높은 것을 Liklihood로 선정해서 한다.









# 빅데이터 실습

## install Weka

- kdd.snu.ac.kr/python/
- 아나콘다와 같이 다운을 받는다.
- 데이터마이닝 툴
- 

## Jupyter Notebook

- ```python
  %matplotlib inline
  
  import pandas as pd
  import matplotlib.pyplot as plt
  
  df = pd.read_csv('cluster2.csv')
  print("Dimensions of the data = {}".format(df.shape))
  
  X = df.values
  X[:5]
  
  plt.figure(figsize=(5, 5))
  plt.scatter(X[:, 0], X[:, 1], s=4)
  plt.show()
  
  cmap = "tab10"
  plt.figure(figsize=(5, 5))
  plt.scatter(X[:, 0], X[:, 1], s=4)
  plt.show()
  
  from sklearn.cluster import KMeans
  k_means = KMeans(n_clusters = 4, random_state = 0)
  y_pred = k_means.fit_predict(X)
  print(y_pred[:10])
  
  plt.figure(figsize=(5,5))
  plt.scatter(X[:, 0], X[:, 1],
             c=y_pred, s=4, cmap=cmap)
  plt.show()
  
  from sklearn.cluster import AgglomerativeClustering
  for i, linkage in enumerate(('single', 'complete')):
      clustering = AgglomerativeClustering(
          linkage = linkage, n_clusters = 4)
      y_pred = clustering.fit_predict(X)
      plt.figure(i + 1, figsize = (5,5))
      plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=4, cmap=cmap)
      plt.title(linkage)
      plt.show()
      
      
  // AgglomerativeClustering
  from sklearn.cluster import AgglomerativeClustering
  for i, linkage in enumerate(('single', 'complete', 'average', 'ward')):
      clustering = AgglomerativeClustering(
          linkage = linkage, n_clusters = 4)
      y_pred = clustering.fit_predict(X)
      plt.figure(i + 1, figsize = (5,5))
      plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=4, cmap=cmap)
      plt.title(linkage)
      plt.show()
  
  // DBSCAN
  from sklearn.cluster import DBSCAN
  dbscan = DBSCAN(eps=0.05, min_samples=20)
  y_pred = dbscan.fit_predict(X)
  print(y_pred[:10])
  
  for i, (eps, min_samples) in enumerate(
  ((0.05, 20), (0.06, 20), (0.06, 15), (0.06, 6))):
      dbscan = DBSCAN(eps=eps, min_samples=min_samples)
      y_pred = dbscan.fit_predict(X)
      plt.figure(i+1, figsize=(5,5))
      plt.scatter(X[:,0], X[:,1], 
                 cmap=cmap, s=4, c=y_pred)
      plt.title(linkage)
      plt.show()
      
  // GaussianMixture
  from sklearn.mixture import GaussianMixture
  em = GaussianMixture(n_components = 4, max_iter = 20, random_state = 0)
  y_pred = em.fit_predict(X)
  plt.figure(figsize=(5,5))
  plt.scatter(X[:, 0], X[:, 1], cmap=cmap, s=4, c=y_pred)
  plt.show()
  ```

  